{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ffosb\\Anaconda3\\envs\\consulting_pocs\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# NLTK Tokenizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Word2Vec Implementation\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import multiprocessing as mp\n",
    "from contextlib import closing\n",
    "from itertools import chain\n",
    "from glob import glob\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of comments found: 10916598\n"
     ]
    }
   ],
   "source": [
    "def load_comments(filepath):\n",
    "    all_files = glob(filepath)\n",
    "    return pd.concat((pd.read_csv(f) for f in all_files))\n",
    "\n",
    "comment_df = load_comments(\"data/nba*\")\n",
    "\n",
    "print(\"Total number of comments found: {}\".format(len(comment_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10916598/10916598 [39:09<00:00, 4645.56it/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_comments(comments):\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    tokenized_comments = []\n",
    "    for comment in tqdm(comments):\n",
    "        tokens = word_tokenize(str(comment).lower())\n",
    "        tokens = [w for w in tokens if not w in stopset]\n",
    "        tokenized_comments.append(tokens)\n",
    "    return tokenized_comments\n",
    "\n",
    "tokenized_comments = tokenize_comments(comment_df['body'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed tokenizing 10916598 comments\n"
     ]
    }
   ],
   "source": [
    "print(\"Completed tokenizing {} comments\".format(len(tokenized_comments)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 151550\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(tokenized_comments, min_count=5, size=300, workers=mp.cpu_count())\n",
    "print(\"Vocabulary Size: {}\".format(len(model.wv.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata file successfully written to: output/metadata.tsv\n"
     ]
    }
   ],
   "source": [
    "def write_files(model, model_filename=\"model.tsv\", metadata_filename=\"metadata.tsv\"):\n",
    "    keys = model.wv.vocab.keys()\n",
    "    with open(model_filename, \"w\") as model_file:\n",
    "        for key in keys:\n",
    "            embedding = list(model[key])\n",
    "            model_file.write('\\t'.join(map(str, embedding)))\n",
    "            model_file.write('\\n')\n",
    "    print(\"Model file successfully written to: {}\".format(model_filename))\n",
    "    \n",
    "    with open(metadata_filename, \"wb\") as metadata_file:\n",
    "        metadata_file.write('Word\\n'.encode())\n",
    "        for key in keys:\n",
    "            metadata_file.write(key.encode('utf-8'))\n",
    "            metadata_file.write('\\n'.encode())\n",
    "    print(\"Metadata file successfully written to: {}\".format(metadata_filename))  \n",
    "    \n",
    "write_files(model, \"output/model.tsv\", \"output/metadata.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"models/nba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thicc Rankings:\n",
      "\n",
      "Lowry: 0.20296446289452752\n"
     ]
    }
   ],
   "source": [
    "def determine_rankings(term, limit=15, sort_desc=True, threshold=0.2):\n",
    "    similarities = {}\n",
    "    for player in ['MJ', 'Magic', 'KD', 'Steph', 'LeBron', 'Harden', 'CP3', 'Giannis', 'Draymond', 'Klay', 'Rose', \\\n",
    "                  'Melo', 'Westbrook', 'Stockton', 'Iverson', 'Embiid', 'Duncan', 'Garnett', 'Pierce', 'Kobe', 'Shaq', \\\n",
    "                  'Kareem', 'Wilt', 'Scalabrine', 'Worthy', 'Malone', 'Ewing', 'Dwight', 'Barkley', 'Lillard', 'Wall', \\\n",
    "                  'Beal', 'Simmons', 'Fultz', 'Lonzo', 'Humphries', 'Odom', 'Bennett', 'Oden', 'Gay', 'Foye', 'Jahlil', \\\n",
    "                  'Emeka', 'MKG', 'Kemba', 'MCW', 'Rondo', 'Blake', 'Nate', 'Iggy', 'Oladipo', 'Tatum', 'Flynn', 'Rubio', \\\n",
    "                  'Wade', 'Jimmy', 'KAT', 'Wiggins', 'Korver', 'Deng', 'Redick', 'Webber', 'Rivers', 'Kwame', 'AD', 'Javale', \\\n",
    "                  'Mozgov', 'Deng', 'Lowry', 'Tristan', 'JR', 'Monta', 'Derozan']:\n",
    "        if sort_desc:\n",
    "            similarities[player] = 1 - abs(model.similarity(player.lower(), term.lower()))\n",
    "        else:\n",
    "            similarities[player] = abs(model.similarity(player.lower(), term.lower()))\n",
    "    output = OrderedDict(sorted(similarities.items(), key = lambda t: t[1]))\n",
    "    print(\"{} Rankings:\".format(term))\n",
    "    print()\n",
    "    for i, key in enumerate(output):\n",
    "        if i < limit:\n",
    "            if sort_desc and 1 - output[key] >= threshold:\n",
    "                print(\"{}: {}\".format(key, 1 - output[key]))\n",
    "            else:\n",
    "                if output[key] <= threshold:\n",
    "                    print(\"{}: {}\".format(key, output[key]))\n",
    "\n",
    "determine_rankings(\"thicc\", limit=10, sort_desc=True, threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basketball\n"
     ]
    }
   ],
   "source": [
    "def doesnt_belong(items):\n",
    "    return model.wv.doesnt_match(items.split())\n",
    "\n",
    "print(doesnt_belong(\"lonzo liangelo lavar lamelo basketball\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('go', 0.48412632942199707),\n",
       " ('gotten', 0.4789046049118042),\n",
       " ('went', 0.4782710671424866),\n",
       " ('resigned', 0.46715933084487915),\n",
       " ('re-signed', 0.4587304890155792),\n",
       " ('stayed', 0.44694089889526367),\n",
       " ('taken', 0.426818311214447),\n",
       " ('done', 0.4104292392730713),\n",
       " ('fallen', 0.3936896026134491),\n",
       " ('contended', 0.39327389001846313)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"gone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vectors = model.wv\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
